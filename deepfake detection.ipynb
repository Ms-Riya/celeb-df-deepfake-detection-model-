{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-17T02:59:51.502518Z",
     "iopub.status.busy": "2025-11-17T02:59:51.502290Z",
     "iopub.status.idle": "2025-11-17T03:02:38.473300Z",
     "shell.execute_reply": "2025-11-17T03:02:38.472488Z",
     "shell.execute_reply.started": "2025-11-17T02:59:51.502500Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing a full environment setup to ensure library compatibility...\n",
      "Installing compatible Torch and Torchvision...\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m434.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "easyocr 1.7.2 requires opencv-python-headless, which is not installed.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mInstalling other required libraries...\n",
      "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'opencv-python-headless' candidate (version 4.8.0.74 at https://files.pythonhosted.org/packages/76/02/f128517f3ade4bb5f71e2afd8461dba70e3f466ce745fa1fd1fade9ad1b7/opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from https://pypi.org/simple/opencv-python-headless/) (requires-python:>=3.6))\n",
      "Reason for being yanked: deprecated, use 4.8.0.76\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\n",
      "âœ… Full environment setup complete.\n",
      "âš ï¸  STOP HERE! Go to Runtime -> Restart Runtime, then continue with next cells.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: ENVIRONMENT SETUP (RUN FIRST, THEN RESTART RUNTIME!)\n",
    "# =============================================================================\n",
    "# IMPORTANT: Run this cell ONCE, then go to the menu and select\n",
    "# \"Runtime\" -> \"Disconnect and delete runtime\" before running anything else.\n",
    "\n",
    "print(\"Performing a full environment setup to ensure library compatibility...\")\n",
    "\n",
    "# Step 1: Uninstall potentially conflicting or broken libraries\n",
    "!pip uninstall -y albumentations albucore torch torchvision opencv-python opencv-python-headless --quiet\n",
    "\n",
    "# Step 2: Install compatible PyTorch and Torchvision for a specific CUDA version\n",
    "print(\"Installing compatible Torch and Torchvision...\")\n",
    "!pip install torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu118 --quiet\n",
    "\n",
    "# Step 3: Install our other pinned dependencies\n",
    "print(\"Installing other required libraries...\")\n",
    "!pip install numpy==1.26.4 --quiet\n",
    "!pip install opencv-python-headless==4.8.0.74 --quiet\n",
    "!pip install imageio-ffmpeg --quiet\n",
    "!pip install scikit-learn --quiet\n",
    "!pip install tqdm --quiet\n",
    "!pip install matplotlib --quiet\n",
    "\n",
    "print(\"\\n Full environment setup complete.\")\n",
    "print(\"  STOP HERE! Go to Runtime -> Restart Runtime, then continue with next cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: VERIFY INSTALLATION (RUN AFTER RESTART)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Verifying installed packages...\")\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "print(f\"âœ… PyTorch: {torch.__version__}\")\n",
    "print(f\"âœ… Torchvision: {torchvision.__version__}\")\n",
    "print(f\"âœ… OpenCV: {cv2.__version__}\")\n",
    "print(f\"âœ… NumPy: {np.__version__}\")\n",
    "print(f\"âœ… CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\" CUDA Version: {torch.version.cuda}\")\n",
    "print(\"\\n All libraries verified! Ready to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T17:29:48.832958Z",
     "iopub.status.busy": "2025-11-28T17:29:48.832728Z",
     "iopub.status.idle": "2025-11-28T17:29:57.276812Z",
     "shell.execute_reply": "2025-11-28T17:29:57.276008Z",
     "shell.execute_reply.started": "2025-11-28T17:29:48.832936Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: IMPORTS & CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "CONFIG = {\n",
    "    'dataset_path': Path(\"/kaggle/input/celeb-df-v2/\"),\n",
    "    'subset_size': None,\n",
    "    'batch_size': 4,\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 1e-4,     # Back to a standard rate\n",
    "    'weight_decay': 1e-4,\n",
    "    'frame_count': 16,\n",
    "    'image_size': 112,\n",
    "    'patience': 10,\n",
    "    'num_workers': 2,\n",
    "    'save_dir': Path('/kaggle/working/celeb_df_results_resnet_baseline'), # New name\n",
    "    'seed': 42,\n",
    "    'warmup_epochs': 3, # Doesn't matter since AFSL is off, but good to reset\n",
    "}\n",
    "\n",
    "print(\"âœ… Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T17:29:57.278637Z",
     "iopub.status.busy": "2025-11-28T17:29:57.278236Z",
     "iopub.status.idle": "2025-11-28T17:29:57.284403Z",
     "shell.execute_reply": "2025-11-28T17:29:57.283602Z",
     "shell.execute_reply.started": "2025-11-28T17:29:57.278618Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Sets the random seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"Returns the appropriate device (CUDA or CPU).\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"âœ… Using CUDA GPU for training.\")\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        print(\"âš ï¸ CUDA not available. Using CPU for training.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "print(\"âœ… Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T17:29:57.285519Z",
     "iopub.status.busy": "2025-11-28T17:29:57.285217Z",
     "iopub.status.idle": "2025-11-28T17:29:57.305420Z",
     "shell.execute_reply": "2025-11-28T17:29:57.304806Z",
     "shell.execute_reply.started": "2025-11-28T17:29:57.285502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Adversarial attack function defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: ADVERSARIAL ATTACK\n",
    "# =============================================================================\n",
    "\n",
    "def pgd_attack(model, inputs, labels, epsilon=4/255, alpha=1/255, num_iter=3):\n",
    "    \"\"\"\n",
    "    Creates adversarial examples using PGD attack.\n",
    "    Reduced strength to prevent overwhelming the model.\n",
    "    \"\"\"\n",
    "    original_inputs = inputs.clone().detach()\n",
    "    adv_inputs = inputs.clone().detach()\n",
    "    adv_inputs.requires_grad = True\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        model.zero_grad()\n",
    "        _, outputs = model(adv_inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        adv_inputs = adv_inputs.detach() + alpha * adv_inputs.grad.sign()\n",
    "        perturbation = torch.clamp(adv_inputs - original_inputs, min=-epsilon, max=epsilon)\n",
    "        adv_inputs = torch.clamp(original_inputs + perturbation, min=-10, max=10).detach()\n",
    "        adv_inputs.requires_grad = True\n",
    "\n",
    "    return adv_inputs\n",
    "\n",
    "print(\"âœ… Adversarial attack function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T17:29:57.306473Z",
     "iopub.status.busy": "2025-11-28T17:29:57.306156Z",
     "iopub.status.idle": "2025-11-28T17:29:57.321967Z",
     "shell.execute_reply": "2025-11-28T17:29:57.321431Z",
     "shell.execute_reply.started": "2025-11-28T17:29:57.306450Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loading classes defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "def load_video_paths(dataset_root: Path) -> Tuple[List[Path], List[Path]]:\n",
    "    \"\"\"Loads video file paths from the Celeb-DF directory structure.\"\"\"\n",
    "    real_videos = list((dataset_root / \"Celeb-real\").glob(\"*.mp4\"))\n",
    "    fake_videos = list((dataset_root / \"Celeb-synthesis\").glob(\"*.mp4\"))\n",
    "    print(f\"Found {len(real_videos):,} REAL videos and {len(fake_videos):,} FAKE videos.\")\n",
    "    print(f\"Class distribution: REAL={len(real_videos)/(len(real_videos)+len(fake_videos))*100:.1f}%, \"\n",
    "          f\"FAKE={len(fake_videos)/(len(real_videos)+len(fake_videos))*100:.1f}%\")\n",
    "    return real_videos, fake_videos\n",
    "\n",
    "class CelebDFDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for loading and processing video frames.\"\"\"\n",
    "    def __init__(self, video_paths: List[Path], labels: List[int], transform=None, frame_count: int = 16):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.frame_count = frame_count\n",
    "        self.failed_videos = 0\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def _sample_frames(self, video_path: Path) -> np.ndarray:\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if not cap.isOpened() or total_frames < self.frame_count:\n",
    "            if cap.isOpened(): cap.release()\n",
    "            self.failed_videos += 1\n",
    "            return np.zeros((self.frame_count, CONFIG['image_size'], CONFIG['image_size'], 3), dtype=np.uint8)\n",
    "\n",
    "        indices = np.linspace(0, total_frames - 1, self.frame_count, dtype=int)\n",
    "        frames = []\n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = cv2.resize(frame, (CONFIG['image_size'], CONFIG['image_size']), interpolation=cv2.INTER_AREA)\n",
    "                frames.append(frame)\n",
    "            else:\n",
    "                frames.append(np.zeros((CONFIG['image_size'], CONFIG['image_size'], 3), dtype=np.uint8))\n",
    "        cap.release()\n",
    "        return np.array(frames)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        frames_np = self._sample_frames(video_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            frames = torch.stack([self.transform(frame) for frame in frames_np])\n",
    "            \n",
    "        return frames, label\n",
    "\n",
    "print(\"âœ… Data loading classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T17:29:57.324286Z",
     "iopub.status.busy": "2025-11-28T17:29:57.323810Z",
     "iopub.status.idle": "2025-11-28T17:29:57.339996Z",
     "shell.execute_reply": "2025-11-28T17:29:57.339413Z",
     "shell.execute_reply.started": "2025-11-28T17:29:57.324269Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model architecture defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: MODEL ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn2(self.pointwise(self.relu(self.bn1(self.depthwise(x))))))\n",
    "\n",
    "class LightweightDeepfakeDetector(nn.Module):\n",
    "    def __init__(self, num_classes=2, dropout_rate=0.6):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            DepthwiseSeparableConv(32, 64, stride=1),\n",
    "            DepthwiseSeparableConv(64, 128, stride=2),\n",
    "            DepthwiseSeparableConv(128, 256, stride=2),\n",
    "            DepthwiseSeparableConv(256, 512, stride=2),\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.temporal_module = nn.LSTM(512, 128, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.shape\n",
    "        x = x.view(b * t, c, h, w)\n",
    "        features = self.backbone(x)\n",
    "        features = self.global_pool(features).view(b, t, -1)\n",
    "        lstm_out, _ = self.temporal_module(features)\n",
    "        temporal_features = torch.cat((lstm_out[:, -1, :128], lstm_out[:, 0, 128:]), dim=-1)\n",
    "        output = self.classifier(temporal_features)\n",
    "        return temporal_features, output\n",
    "\n",
    "print(\"âœ… Model architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T17:29:57.340715Z",
     "iopub.status.busy": "2025-11-28T17:29:57.340563Z",
     "iopub.status.idle": "2025-11-28T17:29:57.361065Z",
     "shell.execute_reply": "2025-11-28T17:29:57.360334Z",
     "shell.execute_reply.started": "2025-11-28T17:29:57.340703Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training function defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: TRAINING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, epochs, save_dir):\n",
    "    \"\"\"Enhanced training loop with warmup and better loss balancing.\"\"\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    classification_criterion = nn.CrossEntropyLoss()\n",
    "    similarity_criterion = nn.CosineSimilarity(dim=1)\n",
    "    \n",
    "    # AFSL hyperparameters (activated after warmup)\n",
    "    beta1 = 0.5   # Reduced from 1.0\n",
    "    beta2 = 0.05  # Reduced from 0.1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Determine if we should use AFSL\n",
    "        use_afsl = False  # <-- TEMPORARILY DISABLE AFSL\n",
    "        if epoch == CONFIG['warmup_epochs']:\n",
    "            print(\" Warmup complete! Activating AFSL...\")\n",
    "        \n",
    "        # --- Training phase ---\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            clean_features, clean_outputs = model(inputs)\n",
    "            loss_dcl = classification_criterion(clean_outputs, labels)\n",
    "            \n",
    "            if use_afsl:\n",
    "                adv_inputs = pgd_attack(model, inputs, labels)\n",
    "                adv_features, _ = model(adv_inputs)\n",
    "                \n",
    "                loss_asl = (1 - similarity_criterion(clean_features, adv_features)).mean()\n",
    "                \n",
    "                real_mask = labels == 0\n",
    "                fake_mask = labels == 1\n",
    "                loss_srl = 0.0\n",
    "                if real_mask.sum() > 0 and fake_mask.sum() > 0:\n",
    "                    real_features = clean_features[real_mask]\n",
    "                    fake_features = clean_features[fake_mask]\n",
    "                    loss_srl = similarity_criterion(\n",
    "                        real_features.mean(dim=0, keepdim=True), \n",
    "                        fake_features.mean(dim=0, keepdim=True)\n",
    "                    ).mean()\n",
    "\n",
    "                total_loss = loss_dcl + (beta1 * loss_asl) + (beta2 * loss_srl)\n",
    "            else:\n",
    "                total_loss = loss_dcl\n",
    "            \n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += total_loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(clean_outputs, 1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "        # --- Validation phase ---\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        val_preds, val_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                _, outputs = model(inputs)\n",
    "                loss = classification_criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # --- Metrics ---\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        avg_train_loss = train_loss / train_total\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        avg_val_loss = val_loss / val_total\n",
    "        \n",
    "        train_cm = confusion_matrix(all_labels, all_preds)\n",
    "        val_cm = confusion_matrix(val_labels, val_preds)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Training:\")\n",
    "        print(f\"   Loss: {avg_train_loss:.4f} | Acc: {train_accuracy:.2f}%\")\n",
    "        print(f\"   Confusion Matrix:\\n{train_cm}\")\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Validation:\")\n",
    "        print(f\"   Loss: {avg_val_loss:.4f} | Acc: {val_accuracy:.2f}%\")\n",
    "        print(f\"   Confusion Matrix:\\n{val_cm}\")\n",
    "\n",
    "        scheduler.step(val_accuracy)\n",
    "\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc, patience_counter = val_accuracy, 0\n",
    "            torch.save(model.state_dict(), save_dir / \"best_model.pth\")\n",
    "            print(f\"\\n Best model saved: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"\\n Patience: {patience_counter}/{CONFIG['patience']}\")\n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(\" Early stopping\")\n",
    "                break\n",
    "\n",
    "print(\"âœ… Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T17:29:57.362096Z",
     "iopub.status.busy": "2025-11-28T17:29:57.361875Z",
     "iopub.status.idle": "2025-11-28T17:29:57.377678Z",
     "shell.execute_reply": "2025-11-28T17:29:57.377069Z",
     "shell.execute_reply.started": "2025-11-28T17:29:57.362075Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Main function defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: MAIN FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    set_seed(CONFIG['seed'])\n",
    "    device = get_device()\n",
    "    \n",
    "    # Load data (THE FULL DATASET)\n",
    "    real_videos, fake_videos = load_video_paths(CONFIG['dataset_path'])\n",
    "    all_videos = real_videos + fake_videos\n",
    "    all_labels = [0] * len(real_videos) + [1] * len(fake_videos)\n",
    "    print(f\"\\nðŸ“‹ Labels: 0=REAL, 1=FAKE\")\n",
    "\n",
    "    # Split (THE FULL DATASET)\n",
    "    train_videos, val_videos, train_labels, val_labels = train_test_split(\n",
    "        all_videos, all_labels, test_size=0.2, stratify=all_labels, random_state=CONFIG['seed']\n",
    "    )\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    train_dataset = CelebDFDataset(train_videos, train_labels, transform=train_transform, frame_count=CONFIG['frame_count'])\n",
    "    val_dataset = CelebDFDataset(val_videos, val_labels, transform=val_transform, frame_count=CONFIG['frame_count'])\n",
    "\n",
    "    # Balanced sampler\n",
    "    class_counts = np.bincount(train_labels)\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = np.array([class_weights[label] for label in train_labels])\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], sampler=sampler, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "\n",
    "    # Model setup\n",
    "    model = LightweightDeepfakeDetector().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\n Starting training...\")\n",
    "    start_time = time.time()\n",
    "    train_model(model, train_loader, val_loader, optimizer, scheduler, device, CONFIG['epochs'], CONFIG['save_dir'])\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nâœ… Done in {(end_time - start_time) / 60:.2f} minutes.\")\n",
    "    \n",
    "    if train_dataset.failed_videos > 0:\n",
    "        print(f\" {train_dataset.failed_videos} videos failed to load.\")\n",
    "\n",
    "print(\"âœ… Main function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T17:29:57.378674Z",
     "iopub.status.busy": "2025-11-28T17:29:57.378461Z",
     "iopub.status.idle": "2025-11-28T17:29:57.403259Z",
     "shell.execute_reply": "2025-11-28T17:29:57.402733Z",
     "shell.execute_reply.started": "2025-11-28T17:29:57.378657Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Focal Loss training function ready!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ADD THIS AS A NEW CELL - FOCAL LOSS & IMPROVED TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance.\"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "def train_model_focal(model, train_loader, val_loader, optimizer, scheduler, device, epochs, save_dir):\n",
    "    \"\"\"Training with Focal Loss and per-class metrics.\"\"\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    best_val_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Focal Loss with higher alpha for minority class\n",
    "    classification_criterion = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "    similarity_criterion = nn.CosineSimilarity(dim=1)\n",
    "    \n",
    "    beta1 = 0.3\n",
    "    beta2 = 0.03\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        use_afsl = epoch >= CONFIG['warmup_epochs']\n",
    "        if epoch == CONFIG['warmup_epochs']:\n",
    "            print(\" Warmup complete! Activating AFSL...\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            clean_features, clean_outputs = model(inputs)\n",
    "            loss_dcl = classification_criterion(clean_outputs, labels)\n",
    "            \n",
    "            if use_afsl:\n",
    "                adv_inputs = pgd_attack(model, inputs, labels)\n",
    "                adv_features, _ = model(adv_inputs)\n",
    "                loss_asl = (1 - similarity_criterion(clean_features, adv_features)).mean()\n",
    "                \n",
    "                real_mask = labels == 0\n",
    "                fake_mask = labels == 1\n",
    "                loss_srl = 0.0\n",
    "                if real_mask.sum() > 0 and fake_mask.sum() > 0:\n",
    "                    real_features = clean_features[real_mask]\n",
    "                    fake_features = clean_features[fake_mask]\n",
    "                    loss_srl = similarity_criterion(\n",
    "                        real_features.mean(dim=0, keepdim=True), \n",
    "                        fake_features.mean(dim=0, keepdim=True)\n",
    "                    ).mean()\n",
    "                total_loss = loss_dcl + (beta1 * loss_asl) + (beta2 * loss_srl)\n",
    "            else:\n",
    "                total_loss = loss_dcl\n",
    "            \n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += total_loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(clean_outputs, 1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        val_preds, val_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                _, outputs = model(inputs)\n",
    "                loss = classification_criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Metrics\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        avg_train_loss = train_loss / train_total\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        avg_val_loss = val_loss / val_total\n",
    "        \n",
    "        train_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "        \n",
    "        train_cm = confusion_matrix(all_labels, all_preds)\n",
    "        val_cm = confusion_matrix(val_labels, val_preds)\n",
    "        \n",
    "        train_real_acc = 100 * train_cm[0, 0] / train_cm[0].sum() if train_cm[0].sum() > 0 else 0\n",
    "        train_fake_acc = 100 * train_cm[1, 1] / train_cm[1].sum() if train_cm[1].sum() > 0 else 0\n",
    "        val_real_acc = 100 * val_cm[0, 0] / val_cm[0].sum() if val_cm[0].sum() > 0 else 0\n",
    "        val_fake_acc = 100 * val_cm[1, 1] / val_cm[1].sum() if val_cm[1].sum() > 0 else 0\n",
    "        \n",
    "        print(f\"\\n Training:\")\n",
    "        print(f\"   Overall - Loss: {avg_train_loss:.4f} | Acc: {train_accuracy:.2f}% | F1: {train_f1:.4f}\")\n",
    "        print(f\"   Per-Class - REAL: {train_real_acc:.2f}% | FAKE: {train_fake_acc:.2f}%\")\n",
    "        print(f\"   Confusion Matrix:\\n{train_cm}\")\n",
    "        \n",
    "        print(f\"\\n Validation:\")\n",
    "        print(f\"   Overall - Loss: {avg_val_loss:.4f} | Acc: {val_accuracy:.2f}% | F1: {val_f1:.4f}\")\n",
    "        print(f\"   Per-Class - REAL: {val_real_acc:.2f}% | FAKE: {val_fake_acc:.2f}%\")\n",
    "        print(f\"   Confusion Matrix:\\n{val_cm}\")\n",
    "\n",
    "        scheduler.step(val_f1)\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1, patience_counter = val_f1, 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_f1': val_f1,\n",
    "                'val_accuracy': val_accuracy,\n",
    "            }, save_dir / \"best_model_focal.pth\")\n",
    "            print(f\"\\nðŸŽ‰ Best F1: {best_val_f1:.4f} (Acc: {val_accuracy:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"\\n Patience: {patience_counter}/{CONFIG['patience']}\")\n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(\" Early stopping\")\n",
    "                break\n",
    "\n",
    "print(\"âœ… Focal Loss training function ready!\")\n",
    "\n",
    "# =============================================================================\n",
    "# RUN THIS CELL TO START FOCAL LOSS TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "def main_with_focal():\n",
    "    \"\"\"Run training with Focal Loss.\"\"\"\n",
    "    set_seed(CONFIG['seed'])\n",
    "    device = get_device()\n",
    "    \n",
    "    # Load data\n",
    "    real_videos, fake_videos = load_video_paths(CONFIG['dataset_path'])\n",
    "    all_videos = real_videos + fake_videos\n",
    "    all_labels = [0] * len(real_videos) + [1] * len(fake_videos)\n",
    "    print(f\"\\nðŸ“‹ Labels: 0=REAL, 1=FAKE\")\n",
    "\n",
    "    # Split\n",
    "    train_videos, val_videos, train_labels, val_labels = train_test_split(\n",
    "        all_videos, all_labels, test_size=0.2, stratify=all_labels, random_state=CONFIG['seed']\n",
    "    )\n",
    "\n",
    "    # Transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    train_dataset = CelebDFDataset(train_videos, train_labels, transform=train_transform, frame_count=CONFIG['frame_count'])\n",
    "    val_dataset = CelebDFDataset(val_videos, val_labels, transform=val_transform, frame_count=CONFIG['frame_count'])\n",
    "\n",
    "    # Balanced sampler\n",
    "    class_counts = np.bincount(train_labels)\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = np.array([class_weights[label] for label in train_labels])\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], sampler=sampler, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "\n",
    "    # Model\n",
    "    model = LightweightDeepfakeDetector().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "    \n",
    "    # Train with Focal Loss\n",
    "    print(f\"\\nðŸš€ Starting training with Focal Loss (alpha=0.75, gamma=2.0)...\")\n",
    "    start_time = time.time()\n",
    "    train_model_focal(model, train_loader, val_loader, optimizer, scheduler, device, CONFIG['epochs'], CONFIG['save_dir'])\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nâœ… Training complete in {(end_time - start_time) / 60:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T17:29:57.404327Z",
     "iopub.status.busy": "2025-11-28T17:29:57.404068Z",
     "iopub.status.idle": "2025-11-28T17:29:57.434896Z",
     "shell.execute_reply": "2025-11-28T17:29:57.434329Z",
     "shell.execute_reply.started": "2025-11-28T17:29:57.404312Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Best practices training function ready!\n",
      "âœ… Improved model with higher dropout ready!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL BEST PRACTICES TRAINING - All Improvements Combined\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "def train_model_best_practices(model, train_loader, val_loader, optimizer, scheduler, device, epochs, save_dir):\n",
    "    \"\"\"\n",
    "    Final optimized training with all best practices.\n",
    "    \"\"\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    best_val_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # IMPROVEMENT 1: Balanced weight for minority REAL class\n",
    "    class_weights = torch.tensor([2.0, 1.0]).to(device)  # Sweet spot for 30-70 split\n",
    "    classification_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # Track history for analysis\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_f1': [],\n",
    "        'train_real_acc': [], 'train_fake_acc': [],\n",
    "        'val_real_acc': [], 'val_fake_acc': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # --- Training phase ---\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            _, outputs = model(inputs)\n",
    "            loss = classification_criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # --- Validation phase ---\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        val_preds, val_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                _, outputs = model(inputs)\n",
    "                loss = classification_criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # --- Calculate metrics ---\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        avg_train_loss = train_loss / train_total\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        avg_val_loss = val_loss / val_total\n",
    "        \n",
    "        train_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "        \n",
    "        train_cm = confusion_matrix(all_labels, all_preds)\n",
    "        val_cm = confusion_matrix(val_labels, val_preds)\n",
    "        \n",
    "        train_real_acc = 100 * train_cm[0, 0] / train_cm[0].sum() if train_cm[0].sum() > 0 else 0\n",
    "        train_fake_acc = 100 * train_cm[1, 1] / train_cm[1].sum() if train_cm[1].sum() > 0 else 0\n",
    "        val_real_acc = 100 * val_cm[0, 0] / val_cm[0].sum() if val_cm[0].sum() > 0 else 0\n",
    "        val_fake_acc = 100 * val_cm[1, 1] / val_cm[1].sum() if val_cm[1].sum() > 0 else 0\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        history['train_f1'].append(train_f1)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['train_real_acc'].append(train_real_acc)\n",
    "        history['train_fake_acc'].append(train_fake_acc)\n",
    "        history['val_real_acc'].append(val_real_acc)\n",
    "        history['val_fake_acc'].append(val_fake_acc)\n",
    "        \n",
    "        # --- Display results ---\n",
    "        print(f\"\\nðŸ“Š Training:\")\n",
    "        print(f\"   Overall - Loss: {avg_train_loss:.4f} | Acc: {train_accuracy:.2f}% | F1: {train_f1:.4f}\")\n",
    "        print(f\"   Per-Class - REAL: {train_real_acc:.2f}% | FAKE: {train_fake_acc:.2f}%\")\n",
    "        print(f\"   Confusion Matrix:\")\n",
    "        print(f\"   [[{train_cm[0, 0]:4d} {train_cm[0, 1]:4d}]   â† REAL predictions\")\n",
    "        print(f\"    [{train_cm[1, 0]:4d} {train_cm[1, 1]:4d}]]  â† FAKE predictions\")\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Validation:\")\n",
    "        print(f\"   Overall - Loss: {avg_val_loss:.4f} | Acc: {val_accuracy:.2f}% | F1: {val_f1:.4f}\")\n",
    "        print(f\"   Per-Class - REAL: {val_real_acc:.2f}% | FAKE: {val_fake_acc:.2f}%\")\n",
    "        print(f\"   Confusion Matrix:\")\n",
    "        print(f\"   [[{val_cm[0, 0]:4d} {val_cm[0, 1]:4d}]   â† REAL predictions\")\n",
    "        print(f\"    [{val_cm[1, 0]:4d} {val_cm[1, 1]:4d}]]  â† FAKE predictions\")\n",
    "\n",
    "        scheduler.step(val_f1)\n",
    "\n",
    "        # --- Save best model ---\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1, patience_counter = val_f1, 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_f1': val_f1,\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'history': history,\n",
    "                'val_cm': val_cm,\n",
    "            }, save_dir / \"best_model_final.pth\")\n",
    "            print(f\"\\n Best F1: {best_val_f1:.4f} (Acc: {val_accuracy:.2f}%)\")\n",
    "            print(f\"   REAL: {val_real_acc:.2f}% | FAKE: {val_fake_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"\\n Patience: {patience_counter}/{CONFIG['patience']}\")\n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(\" Early stopping\")\n",
    "                break\n",
    "    \n",
    "    # --- Final report ---\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ðŸ“ˆ TRAINING SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Best Validation F1: {best_val_f1:.4f}\")\n",
    "    print(f\"Best Validation Accuracy: {max(history['val_acc']):.2f}%\")\n",
    "    print(f\"Best REAL Accuracy: {max(history['val_real_acc']):.2f}%\")\n",
    "    print(f\"Best FAKE Accuracy: {max(history['val_fake_acc']):.2f}%\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"âœ… Best practices training function ready!\")\n",
    "\n",
    "# =============================================================================\n",
    "# IMPROVED MODEL WITH HIGHER DROPOUT\n",
    "# =============================================================================\n",
    "\n",
    "class ImprovedDeepfakeDetector(nn.Module):\n",
    "    \"\"\"Enhanced model with higher dropout to reduce overfitting.\"\"\"\n",
    "    def __init__(self, num_classes=2, dropout_rate=0.6): \n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            DepthwiseSeparableConv(32, 64, stride=1),\n",
    "            DepthwiseSeparableConv(64, 128, stride=2),\n",
    "            DepthwiseSeparableConv(128, 256, stride=2),\n",
    "            DepthwiseSeparableConv(256, 512, stride=2),\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.temporal_module = nn.LSTM(512, 128, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Dropout(dropout_rate),  # Higher dropout\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.shape\n",
    "        x = x.view(b * t, c, h, w)\n",
    "        features = self.backbone(x)\n",
    "        features = self.global_pool(features).view(b, t, -1)\n",
    "        lstm_out, _ = self.temporal_module(features)\n",
    "        temporal_features = torch.cat((lstm_out[:, -1, :128], lstm_out[:, 0, 128:]), dim=-1)\n",
    "        output = self.classifier(temporal_features)\n",
    "        return temporal_features, output\n",
    "\n",
    "print(\"âœ… Improved model with higher dropout ready!\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL MAIN FUNCTION WITH ALL IMPROVEMENTS\n",
    "# =============================================================================\n",
    "\n",
    "def main_final_best():\n",
    "    \"\"\"\n",
    "    Final training with all best practices:\n",
    "    - Balanced dataset (30-70 split)\n",
    "    - Stronger augmentation\n",
    "    - Higher class weights (3.5x for REAL)\n",
    "    - Higher dropout (0.6)\n",
    "    - Lower learning rate (5e-5)\n",
    "    \"\"\"\n",
    "    set_seed(CONFIG['seed'])\n",
    "    device = get_device()\n",
    "    \n",
    "    # Load and balance data\n",
    "    real_videos, fake_videos = load_video_paths(CONFIG['dataset_path'])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ðŸ”§ CREATING BALANCED DATASET\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    n_real = len(real_videos)\n",
    "    target_ratio = 0.30\n",
    "    n_fake_keep = int(n_real / target_ratio) - n_real\n",
    "    \n",
    "    np.random.seed(CONFIG['seed'])\n",
    "    fake_indices = np.random.choice(len(fake_videos), size=n_fake_keep, replace=False)\n",
    "    fake_videos_sampled = [fake_videos[i] for i in fake_indices]\n",
    "    \n",
    "    all_videos = real_videos + fake_videos_sampled\n",
    "    all_labels = [0] * len(real_videos) + [1] * len(fake_videos_sampled)\n",
    "    \n",
    "    print(f\"âœ… Balanced dataset: {len(real_videos)} REAL ({30.0:.1f}%), \"\n",
    "          f\"{len(fake_videos_sampled)} FAKE ({70.0:.1f}%)\")\n",
    "\n",
    "    # Split\n",
    "    train_videos, val_videos, train_labels, val_labels = train_test_split(\n",
    "        all_videos, all_labels, test_size=0.2, stratify=all_labels, random_state=CONFIG['seed']\n",
    "    )\n",
    "    \n",
    "    print(f\"Training: {len(train_videos)} | Validation: {len(val_videos)}\")\n",
    "\n",
    "    # IMPROVEMENT 2: Stronger augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # Geometric augmentations\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        # Color augmentations\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        # Normalize\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        # Random erasing\n",
    "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)),\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    train_dataset = CelebDFDataset(train_videos, train_labels, transform=train_transform, frame_count=CONFIG['frame_count'])\n",
    "    val_dataset = CelebDFDataset(val_videos, val_labels, transform=val_transform, frame_count=CONFIG['frame_count'])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "\n",
    "    # IMPROVEMENT 3: Use improved model with higher dropout\n",
    "    model = ImprovedDeepfakeDetector(dropout_rate=0.6).to(device)\n",
    "    \n",
    "    # IMPROVEMENT 4: Lower learning rate for better generalization\n",
    "    lr = 5e-5  # Reduced from 1e-4\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=CONFIG['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "    \n",
    "    print(f\"\\nðŸš€ FINAL TRAINING WITH ALL IMPROVEMENTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"âœ… Balanced dataset (30-70 split)\")\n",
    "    print(f\"âœ… Stronger augmentation (RandomAffine, Grayscale, Erasing)\")\n",
    "    print(f\"âœ… Higher class weight (REAL=2.0x, FAKE=1.0x)\")\n",
    "    print(f\"âœ… Higher dropout (0.6)\")\n",
    "    print(f\"âœ… Lower learning rate ({lr})\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = train_model_best_practices(model, train_loader, val_loader, optimizer, scheduler, device, CONFIG['epochs'], CONFIG['save_dir'])\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\nâœ… Training complete in {(end_time - start_time) / 60:.2f} minutes.\")\n",
    "    \n",
    "    # Save training history\n",
    "    import json\n",
    "    with open(CONFIG['save_dir'] / 'training_history.json', 'w') as f:\n",
    "        json.dump({k: [float(v) if isinstance(v, (int, float, np.number)) else v for v in vals] \n",
    "                   for k, vals in history.items()}, f, indent=2)\n",
    "    print(f\"ðŸ“Š Training history saved to {CONFIG['save_dir'] / 'training_history.json'}\")\n",
    "\n",
    "# =============================================================================\n",
    "# RUN THE FINAL OPTIMIZED TRAINING!\n",
    "# ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T17:29:57.435871Z",
     "iopub.status.busy": "2025-11-28T17:29:57.435615Z",
     "iopub.status.idle": "2025-11-28T17:29:57.851818Z",
     "shell.execute_reply": "2025-11-28T17:29:57.851112Z",
     "shell.execute_reply.started": "2025-11-28T17:29:57.435855Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading weights file manually...\n",
      "--2025-11-28 17:29:57--  https://download.pytorch.org/models/efficientnet_b0_rwightman-3dd342df.pth\n",
      "Resolving download.pytorch.org (download.pytorch.org)... 18.160.200.71, 18.160.200.126, 18.160.200.112, ...\n",
      "Connecting to download.pytorch.org (download.pytorch.org)|18.160.200.71|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 21444401 (20M) [binary/octet-stream]\n",
      "Saving to: â€˜/kaggle/working/efficientnet_b0_weights.pthâ€™\n",
      "\n",
      "/kaggle/working/eff 100%[===================>]  20.45M   133MB/s    in 0.2s    \n",
      "\n",
      "2025-11-28 17:29:57 (133 MB/s) - â€˜/kaggle/working/efficientnet_b0_weights.pthâ€™ saved [21444401/21444401]\n",
      "\n",
      "âœ… Weights downloaded to /kaggle/working/efficientnet_b0_weights.pth\n",
      "You can now run the modified main cell (Step 2).\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Manually download the weights\n",
    "print(\"Downloading weights file manually...\")\n",
    "!wget https://download.pytorch.org/models/efficientnet_b0_rwightman-3dd342df.pth -O /kaggle/working/efficientnet_b0_weights.pth\n",
    "print(\"âœ… Weights downloaded to /kaggle/working/efficientnet_b0_weights.pth\")\n",
    "print(\"You can now run the modified main cell (Step 2).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T17:29:57.853159Z",
     "iopub.status.busy": "2025-11-28T17:29:57.852950Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 590 REAL videos and 5,639 FAKE videos.\n",
      "Class distribution: REAL=9.5%, FAKE=90.5%\n",
      "\n",
      "======================================================================\n",
      "TWO-STREAM EFFICIENTNET-B0 WITH MOSAIC (AFSL Corrected + Checkpointing)\n",
      "======================================================================\n",
      "Dataset: 590 REAL, 5639 FAKE\n",
      "Evaluation Metric: AUC (Area Under ROC Curve)\n",
      "\n",
      "Attempting to load weights from local file: /kaggle/working/efficientnet_b0_weights.pth\n",
      "âœ… Weights loaded successfully from local file.\n",
      "Starting a new training run.\n",
      "\n",
      "ðŸš€ Starting Training...\n",
      "   Temporal Stream: 16 frames â†’ EfficientNet-B0 â†’ LSTM\n",
      "   Spatial Stream: 4Ã—4 mosaic â†’ EfficientNet-B0\n",
      "   Fusion: Concatenate â†’ Classifier\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Epoch 1/50\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1246/1246 [12:17<00:00,  1.69it/s]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 312/312 [02:43<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Training:   Loss: 0.6899 | AUC: 0.5977\n",
      "ðŸ“Š Validation: Loss: 0.5006 | AUC: 0.6126\n",
      "ðŸŽ‰ New best AUC: 0.6126 (Saved to /kaggle/working/celeb_df_results_resnet_baseline/best_model_two_stream.pth)\n",
      "ðŸ’¾ Checkpoint saved to /kaggle/working/celeb_df_results_resnet_baseline/resume_checkpoint.pth (can resume from epoch 1)\n",
      "\n",
      "======================================================================\n",
      "Epoch 2/50\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 866/1246 [08:22<03:10,  1.99it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# TWO-STREAM EFFICIENTNET-B0 FOR DEEPFAKE DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os # Import os to check for files\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET WITH MOSAIC GENERATION (No changes)\n",
    "# =============================================================================\n",
    "\n",
    "class CelebDFWithMosaic(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that returns:\n",
    "    - 16 individual frames\n",
    "    - 1 mosaic image (4x4 grid of all 16 frames)\n",
    "    \"\"\"\n",
    "    def __init__(self, video_paths, labels, transform=None, frame_count=16):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.frame_count = frame_count\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def _sample_frames(self, video_path):\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if not cap.isOpened() or total_frames < self.frame_count:\n",
    "            if cap.isOpened(): cap.release()\n",
    "            return np.zeros((self.frame_count, CONFIG['image_size'], CONFIG['image_size'], 3), dtype=np.uint8)\n",
    "        indices = np.linspace(0, total_frames - 1, self.frame_count, dtype=int)\n",
    "        frames = []\n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = cv2.resize(frame, (CONFIG['image_size'], CONFIG['image_size']))\n",
    "                frames.append(frame)\n",
    "            else:\n",
    "                frames.append(np.zeros((CONFIG['image_size'], CONFIG['image_size'], 3), dtype=np.uint8))\n",
    "        cap.release()\n",
    "        return np.array(frames)\n",
    "\n",
    "    def _create_mosaic(self, frames):\n",
    "        img_size = CONFIG['image_size']\n",
    "        mosaic = np.zeros((img_size * 4, img_size * 4, 3), dtype=np.uint8)\n",
    "        for idx, frame in enumerate(frames):\n",
    "            row = idx // 4\n",
    "            col = idx % 4\n",
    "            mosaic[row*img_size:(row+1)*img_size, col*img_size:(col+1)*img_size] = frame\n",
    "        return mosaic\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        frames_np = self._sample_frames(video_path)\n",
    "        mosaic_np = self._create_mosaic(frames_np)\n",
    "        if self.transform:\n",
    "            frames = torch.stack([self.transform(frame) for frame in frames_np])\n",
    "            mosaic = self.transform(mosaic_np)\n",
    "        return frames, mosaic, label\n",
    "\n",
    "# =============================================================================\n",
    "# TWO-STREAM EFFICIENTNET-B0 MODEL (Manual Weight Loading)\n",
    "# =============================================================================\n",
    "\n",
    "class TwoStreamEfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, pretrained=True):\n",
    "        super().__init__()\n",
    "        weights_path = \"/kaggle/working/efficientnet_b0_weights.pth\"\n",
    "        efficientnet_temporal = models.efficientnet_b0(weights=None)\n",
    "        efficientnet_spatial = models.efficientnet_b0(weights=None)\n",
    "        \n",
    "        if pretrained:\n",
    "            print(f\"Attempting to load weights from local file: {weights_path}\")\n",
    "            try:\n",
    "                # Load onto the correct device\n",
    "                state_dict = torch.load(weights_path, map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "                efficientnet_temporal.load_state_dict(state_dict)\n",
    "                efficientnet_spatial.load_state_dict(state_dict)\n",
    "                print(\"âœ… Weights loaded successfully from local file.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ðŸ›‘ FAILED to load weights: {e}\")\n",
    "            \n",
    "        self.temporal_backbone = nn.Sequential(*list(efficientnet_temporal.children())[:-1])\n",
    "        self.spatial_backbone = nn.Sequential(*list(efficientnet_spatial.children())[:-1])\n",
    "        self.feature_dim = 1280\n",
    "        self.temporal_lstm = nn.LSTM(self.feature_dim, 256, batch_first=True, bidirectional=True)\n",
    "        self.spatial_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fusion_classifier = nn.Sequential(\n",
    "            nn.Linear(512 + self.feature_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Using the dropout value from your successful run\n",
    "            nn.Dropout(0.6), \n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, frames, mosaic):\n",
    "        batch_size = frames.size(0)\n",
    "        num_frames = frames.size(1)\n",
    "        frames_flat = frames.view(batch_size * num_frames, *frames.shape[2:])\n",
    "        temporal_features = self.temporal_backbone(frames_flat)\n",
    "        temporal_features = temporal_features.view(batch_size, num_frames, self.feature_dim)\n",
    "        lstm_out, _ = self.temporal_lstm(temporal_features)\n",
    "        temporal_out = lstm_out[:, -1, :]\n",
    "        spatial_features = self.spatial_backbone(mosaic)\n",
    "        spatial_features = self.spatial_pool(spatial_features)\n",
    "        spatial_out = spatial_features.view(batch_size, self.feature_dim)\n",
    "        fused_features = torch.cat([temporal_out, spatial_out], dim=1)\n",
    "        output = self.fusion_classifier(fused_features)\n",
    "        return fused_features, output\n",
    "\n",
    "# =============================================================================\n",
    "# PGD ADVERSARIAL ATTACK (Corrected)\n",
    "# =============================================================================\n",
    "\n",
    "def pgd_attack(model, frames, mosaic, labels, epsilon=4/255, alpha=1/255, num_iter=1): # Using num_iter=1\n",
    "    frames_adv = frames.clone().detach()\n",
    "    mosaic_adv = mosaic.clone().detach()\n",
    "    frames_adv.requires_grad = True\n",
    "    mosaic_adv.requires_grad = True\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        model.zero_grad()\n",
    "        _, outputs = model(frames_adv, mosaic_adv)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        frames_adv = frames_adv.detach() + alpha * frames_adv.grad.sign()\n",
    "        mosaic_adv = mosaic_adv.detach() + alpha * mosaic_adv.grad.sign()\n",
    "        frames_perturbation = torch.clamp(frames_adv - frames, min=-epsilon, max=epsilon)\n",
    "        mosaic_perturbation = torch.clamp(mosaic_adv - mosaic, min=-epsilon, max=epsilon)\n",
    "        frames_adv = torch.clamp(frames + frames_perturbation, min=-10, max=10).detach()\n",
    "        mosaic_adv = torch.clamp(mosaic + mosaic_perturbation, min=-10, max=10).detach()\n",
    "        frames_adv.requires_grad = True\n",
    "        mosaic_adv.requires_grad = True\n",
    "    return frames_adv, mosaic_adv\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING WITH AUC TRACKING + AFSL (MODIFIED FOR CHECKPOINTING)\n",
    "# =============================================================================\n",
    "\n",
    "def train_with_auc(\n",
    "    model, train_loader, val_loader, optimizer, scheduler, device, epochs, save_dir,\n",
    "    # --- NEW: Parameters to allow resuming ---\n",
    "    start_epoch,\n",
    "    best_auc,\n",
    "    patience_counter,\n",
    "    history\n",
    "):\n",
    "    \"\"\"Training loop with AFSL, AUC tracking, and Checkpointing.\"\"\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    similarity_criterion = nn.CosineSimilarity(dim=1)\n",
    "    \n",
    "    # --- NEW: Define checkpoint paths ---\n",
    "    best_model_path = save_dir / \"best_model_two_stream.pth\"\n",
    "    resume_checkpoint_path = save_dir / \"resume_checkpoint.pth\"\n",
    "    \n",
    "    # AFSL hyperparameters (from your successful run)\n",
    "    beta1 = 0.03\n",
    "    beta2 = 0.003\n",
    "    warmup_epochs = 3\n",
    "    \n",
    "    # --- MODIFIED: Start loop from 'start_epoch' ---\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\") # Add +1 for 1-based display\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        use_afsl = epoch >= warmup_epochs\n",
    "        if epoch == warmup_epochs and start_epoch <= warmup_epochs:\n",
    "            print(\"ðŸ”¥ Warmup complete! Activating AFSL...\")\n",
    "        \n",
    "        # ===== TRAINING =====\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        all_preds_probs, all_labels = [], []\n",
    "        \n",
    "        for frames, mosaic, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            frames, mosaic, labels = frames.to(device), mosaic.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            clean_features, clean_outputs = model(frames, mosaic)\n",
    "            loss_dcl = criterion(clean_outputs, labels)\n",
    "            \n",
    "            if use_afsl:\n",
    "                frames_adv, mosaic_adv = pgd_attack(model, frames, mosaic, labels)\n",
    "                adv_features, _ = model(frames_adv, mosaic_adv)\n",
    "                loss_asl = (1 - similarity_criterion(clean_features, adv_features)).mean()\n",
    "                real_mask, fake_mask = labels == 0, labels == 1\n",
    "                loss_srl = 0.0\n",
    "                if real_mask.sum() > 0 and fake_mask.sum() > 0:\n",
    "                    real_features = clean_features[real_mask]\n",
    "                    fake_features = clean_features[fake_mask]\n",
    "                    loss_srl = similarity_criterion(\n",
    "                        real_features.mean(dim=0, keepdim=True),\n",
    "                        fake_features.mean(dim=0, keepdim=True)\n",
    "                    ).mean()\n",
    "                total_loss = loss_dcl + (beta1 * loss_asl) + (beta2 * loss_srl)\n",
    "            else:\n",
    "                total_loss = loss_dcl\n",
    "            \n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += total_loss.item() * frames.size(0)\n",
    "            probs = torch.softmax(clean_outputs, dim=1)[:, 1]\n",
    "            all_preds_probs.extend(probs.detach().cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "        try:\n",
    "            train_auc = roc_auc_score(all_labels, all_preds_probs)\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: Could not calculate train AUC. {e}\")\n",
    "            train_auc = 0.5\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # ===== VALIDATION =====\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds_probs, val_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for frames, mosaic, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "                frames, mosaic, labels = frames.to(device), mosaic.to(device), labels.to(device)\n",
    "                _, outputs = model(frames, mosaic)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * frames.size(0)\n",
    "                probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "                val_preds_probs.extend(probs.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        try:\n",
    "            val_auc = roc_auc_score(val_labels, val_preds_probs)\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: Could not calculate validation AUC. {e}\")\n",
    "            val_auc = 0.5\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        \n",
    "        history['train_auc'].append(train_auc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Training:   Loss: {avg_train_loss:.4f} | AUC: {train_auc:.4f}\")\n",
    "        print(f\"ðŸ“Š Validation: Loss: {avg_val_loss:.4f} | AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_auc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1, # Save as 1-based epoch\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_auc': val_auc,\n",
    "            }, best_model_path)\n",
    "            print(f\"ðŸŽ‰ New best AUC: {best_auc:.4f} (Saved to {best_model_path})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"â³ Patience: {patience_counter}/{CONFIG['patience']}\")\n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(\"ðŸ›‘ Early stopping\")\n",
    "                break\n",
    "        \n",
    "        # --- NEW: Save a resume checkpoint EVERY epoch ---\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1, # Save the *next* epoch to start from\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_auc': best_auc,\n",
    "            'patience_counter': patience_counter,\n",
    "            'history': history,\n",
    "        }, resume_checkpoint_path)\n",
    "        print(f\"ðŸ’¾ Checkpoint saved to {resume_checkpoint_path} (can resume from epoch {epoch + 1})\")\n",
    "    \n",
    "    return history, best_auc\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING FUNCTION (MODIFIED FOR CHECKPOINTING)\n",
    "# =============================================================================\n",
    "\n",
    "def main_two_stream():\n",
    "    set_seed(CONFIG['seed'])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    real_videos, fake_videos = load_video_paths(CONFIG['dataset_path'])\n",
    "    all_videos = real_videos + fake_videos\n",
    "    all_labels = [0] * len(real_videos) + [1] * len(fake_videos)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TWO-STREAM EFFICIENTNET-B0 WITH MOSAIC (AFSL Corrected + Checkpointing)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Dataset: {len(real_videos)} REAL, {len(fake_videos)} FAKE\")\n",
    "    print(f\"Evaluation Metric: AUC (Area Under ROC Curve)\\n\")\n",
    "\n",
    "    train_videos, val_videos, train_labels, val_labels = train_test_split(\n",
    "        all_videos, all_labels, test_size=0.2, stratify=all_labels, random_state=CONFIG['seed']\n",
    "    )\n",
    "\n",
    "    # Adding the augmentations to fight overfitting\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomAffine(degrees=10, scale=(0.9, 1.1)), # Added\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)), # Added\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    train_dataset = CelebDFWithMosaic(train_videos, train_labels, transform=train_transform, frame_count=CONFIG['frame_count'])\n",
    "    val_dataset = CelebDFWithMosaic(val_videos, val_labels, transform=val_transform, frame_count=CONFIG['frame_count'])\n",
    "\n",
    "    class_counts = np.bincount(train_labels)\n",
    "    class_weights = 1. / class_counts\n",
    "    sample_weights = np.array([class_weights[label] for label in train_labels])\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    \n",
    "    # Make sure num_workers=0 in CONFIG cell!\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], sampler=sampler, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "\n",
    "    # Model\n",
    "    model = TwoStreamEfficientNet(pretrained=True).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "    \n",
    "    # --- NEW: CHECKPOINT LOADING LOGIC ---\n",
    "    start_epoch = 0\n",
    "    best_auc = 0.0\n",
    "    patience_counter = 0\n",
    "    history = {'train_auc': [], 'val_auc': [], 'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    resume_checkpoint_path = CONFIG['save_dir'] / \"resume_checkpoint.pth\"\n",
    "\n",
    "    if os.path.exists(resume_checkpoint_path):\n",
    "        print(f\"Resuming training from checkpoint: {resume_checkpoint_path}\")\n",
    "        checkpoint = torch.load(resume_checkpoint_path, map_location=device)\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        start_epoch = checkpoint['epoch'] # Already saved as next epoch\n",
    "        best_auc = checkpoint['best_auc']\n",
    "        patience_counter = checkpoint['patience_counter']\n",
    "        history = checkpoint['history']\n",
    "        \n",
    "        print(f\"âœ… Resumed successfully. Starting from epoch {start_epoch + 1}.\") # +1 for display\n",
    "    else:\n",
    "        print(\"Starting a new training run.\")\n",
    "    # --- END NEW LOGIC ---\n",
    "\n",
    "    print(f\"\\nðŸš€ Starting Training...\")\n",
    "    print(f\"   Temporal Stream: 16 frames â†’ EfficientNet-B0 â†’ LSTM\")\n",
    "    print(f\"   Spatial Stream: 4Ã—4 mosaic â†’ EfficientNet-B0\")\n",
    "    print(f\"   Fusion: Concatenate â†’ Classifier\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history, best_auc = train_with_auc(\n",
    "        model, train_loader, val_loader, optimizer, scheduler, device, CONFIG['epochs'], CONFIG['save_dir'],\n",
    "        # Pass the new state variables\n",
    "        start_epoch=start_epoch,\n",
    "        best_auc=best_auc,\n",
    "        patience_counter=patience_counter,\n",
    "        history=history\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"âœ… Training Complete\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Time: {(end_time - start_time) / 60:.1f} minutes\")\n",
    "    print(f\"Best Validation AUC: {best_auc:.4f}\")\n",
    "\n",
    "# This will run the training\n",
    "main_two_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL INFERENCE DEMO CELL\n",
    "# 1. Run this cell.\n",
    "# 2. Click \"Choose Files\" and upload a video.\n",
    "# 3. The prediction will appear below.\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# --- 1. Re-define all necessary classes and functions ---\n",
    "# We must re-define the model architecture so PyTorch can load the weights.\n",
    "\n",
    "# Ensure CONFIG is available, or define the necessary parts\n",
    "try:\n",
    "    IMAGE_SIZE = CONFIG['image_size']\n",
    "    FRAME_COUNT = CONFIG['frame_count']\n",
    "except NameError:\n",
    "    print(\"CONFIG not found, using default values.\")\n",
    "    IMAGE_SIZE = 112\n",
    "    FRAME_COUNT = 16\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TwoStreamEfficientNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-stream architecture (re-defined from main script).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=2, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        weights_path = \"/kaggle/working/efficientnet_b0_weights.pth\"\n",
    "        \n",
    "        efficientnet_temporal = models.efficientnet_b0(weights=None)\n",
    "        efficientnet_spatial = models.efficientnet_b0(weights=None)\n",
    "        \n",
    "        if pretrained:\n",
    "            print(f\"Loading pre-trained weights from local file: {weights_path}\")\n",
    "            try:\n",
    "                state_dict = torch.load(weights_path, map_location=DEVICE)\n",
    "                efficientnet_temporal.load_state_dict(state_dict)\n",
    "                efficientnet_spatial.load_state_dict(state_dict)\n",
    "                print(\"âœ… EfficientNet backbones loaded.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ðŸ›‘ FAILED to load weights: {e}\")\n",
    "        \n",
    "        self.temporal_backbone = nn.Sequential(*list(efficientnet_temporal.children())[:-1])\n",
    "        self.spatial_backbone = nn.Sequential(*list(efficientnet_spatial.children())[:-1])\n",
    "        self.feature_dim = 1280\n",
    "        self.temporal_lstm = nn.LSTM(self.feature_dim, 256, batch_first=True, bidirectional=True)\n",
    "        self.spatial_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fusion_classifier = nn.Sequential(\n",
    "            nn.Linear(512 + self.feature_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6), # Using the 0.6 dropout from our best model\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, frames, mosaic):\n",
    "        batch_size = frames.size(0)\n",
    "        num_frames = frames.size(1)\n",
    "        \n",
    "        frames_flat = frames.view(batch_size * num_frames, *frames.shape[2:])\n",
    "        temporal_features = self.temporal_backbone(frames_flat)\n",
    "        temporal_features = temporal_features.view(batch_size, num_frames, self.feature_dim)\n",
    "        lstm_out, _ = self.temporal_lstm(temporal_features)\n",
    "        temporal_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        spatial_features = self.spatial_backbone(mosaic)\n",
    "        spatial_features = self.spatial_pool(spatial_features)\n",
    "        spatial_out = spatial_features.view(batch_size, self.feature_dim)\n",
    "        \n",
    "        fused_features = torch.cat([temporal_out, spatial_out], dim=1)\n",
    "        output = self.fusion_classifier(fused_features)\n",
    "        \n",
    "        return fused_features, output\n",
    "\n",
    "# --- 2. Re-define preprocessing functions ---\n",
    "\n",
    "def _sample_frames(video_path):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    if not cap.isOpened() or total_frames < FRAME_COUNT:\n",
    "        if cap.isOpened(): cap.release()\n",
    "        print(f\"Error: Video {video_path} could not be opened or has < {FRAME_COUNT} frames.\")\n",
    "        return None\n",
    "\n",
    "    indices = np.linspace(0, total_frames - 1, FRAME_COUNT, dtype=int)\n",
    "    frames = []\n",
    "    for idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            frames.append(np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8))\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def _create_mosaic(frames):\n",
    "    mosaic = np.zeros((IMAGE_SIZE * 4, IMAGE_SIZE * 4, 3), dtype=np.uint8)\n",
    "    for idx, frame in enumerate(frames):\n",
    "        row = idx // 4\n",
    "        col = idx % 4\n",
    "        mosaic[row*IMAGE_SIZE:(row+1)*IMAGE_SIZE, col*IMAGE_SIZE:(col+1)*IMAGE_SIZE] = frame\n",
    "    return mosaic\n",
    "\n",
    "# Define the *validation* transform (no augmentations)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# --- 3. Load Our Trained Model ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING TRAINED DEEPFAKE DETECTOR...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize model\n",
    "model = TwoStreamEfficientNet(pretrained=True).to(DEVICE)\n",
    "\n",
    "# Load the saved weights from our best run\n",
    "MODEL_PATH = \"/kaggle/working/celeb_df_results_resnet_baseline/best_model_two_stream.pth\"\n",
    "try:\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"âœ… Successfully loaded best model weights from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"   (Achieved {checkpoint['val_auc']:.4f} AUC during training)\")\n",
    "except Exception as e:\n",
    "    print(f\"ðŸ›‘ FAILED to load trained model weights from {MODEL_PATH}\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(\"   Please ensure you have run the training cell first!\")\n",
    "\n",
    "model.eval() # Set model to evaluation mode\n",
    "\n",
    "# --- 4. Define the Inference Function ---\n",
    "\n",
    "def predict_video(video_path):\n",
    "    \"\"\"\n",
    "    Takes a path to a single video, preprocesses it, and runs inference.\n",
    "    \"\"\"\n",
    "    # Preprocess the video\n",
    "    frames_np = _sample_frames(video_path)\n",
    "    if frames_np is None:\n",
    "        return \"Error\", 0.0, \"Could not process video.\"\n",
    "\n",
    "    mosaic_np = _create_mosaic(frames_np)\n",
    "    \n",
    "    # Apply transforms\n",
    "    frames = torch.stack([val_transform(frame) for frame in frames_np])\n",
    "    mosaic = val_transform(mosaic_np)\n",
    "    \n",
    "    # Add batch dimension (batch size = 1)\n",
    "    frames = frames.unsqueeze(0).to(DEVICE)\n",
    "    mosaic = mosaic.unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        _, outputs = model(frames, mosaic)\n",
    "        \n",
    "        # Get probabilities\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        \n",
    "        # Get confidence and prediction\n",
    "        # [0] is REAL, [1] is FAKE\n",
    "        confidence_fake = probabilities[0][1].item()\n",
    "        confidence_real = probabilities[0][0].item()\n",
    "        \n",
    "        if confidence_fake > confidence_real:\n",
    "            prediction = \"FAKE\"\n",
    "            confidence = confidence_fake\n",
    "            caption = f\"The model is {confidence*100:.2f}% sure this video is FAKE.\"\n",
    "        else:\n",
    "            prediction = \"REAL\"\n",
    "            confidence = confidence_real\n",
    "            caption = f\"The model is {confidence*100:.2f}% sure this video is REAL.\"\n",
    "            \n",
    "        return prediction, confidence, caption\n",
    "\n",
    "# --- 5. Create the UI (ipywidgets) ---\n",
    "\n",
    "# Output widget to display results\n",
    "out = widgets.Output(layout={'border': '1px solid black', 'padding': '10px'})\n",
    "\n",
    "# File uploader widget\n",
    "uploader = widgets.FileUpload(\n",
    "    accept='video/*',\n",
    "    multiple=False,\n",
    "    description='Upload Video'\n",
    ")\n",
    "\n",
    "# Event handler for when a file is uploaded\n",
    "def on_upload_change(change):\n",
    "    with out:\n",
    "        clear_output(wait=True) # Clear previous results\n",
    "        print(\"File uploaded. Processing...\")\n",
    "        \n",
    "        # Get uploaded file data\n",
    "        uploaded_file = list(change['new'].values())[0]\n",
    "        content = uploaded_file['content']\n",
    "        name = uploaded_file['name']\n",
    "        \n",
    "        # Save temporary file to disk\n",
    "        temp_path = f\"/kaggle/working/{name}\"\n",
    "        with open(temp_path, 'wb') as f:\n",
    "            f.write(content)\n",
    "            \n",
    "        print(f\"Running detection on {name}...\")\n",
    "        \n",
    "        # Run prediction\n",
    "        start_time = time.time()\n",
    "        prediction, confidence, caption = predict_video(temp_path)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        print(\"  DETECTION RESULTS\")\n",
    "        print(\"=\"*30)\n",
    "        print(f\"  Prediction: {prediction}\")\n",
    "        print(f\"  Confidence: {confidence*100:.2f}%\")\n",
    "        print(f\"  Analysis:   {caption}\")\n",
    "        print(f\"  (Time taken: {end_time - start_time:.2f} seconds)\")\n",
    "\n",
    "# Link the event handler to the uploader\n",
    "uploader.observe(on_upload_change, names='value')\n",
    "\n",
    "# Display the UI\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  LIVE DEEPFAKE DETECTOR\")\n",
    "print(\"=\"*70)\n",
    "print(\"Upload a video file to run a real-time prediction.\")\n",
    "display(uploader, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL INFERENCE DEMO CELL (PERMANENT)\n",
    "#\n",
    "# INSTRUCTIONS:\n",
    "# 1. DO NOT run the 6.5-hour training cell.\n",
    "# 2. Add your saved Kaggle Model (e.g., \"my-deepfake-detector-v1\")\n",
    "#    using the \"Add Data\" button in the sidebar.\n",
    "# 3. Update the `MODEL_INPUT_PATH` variable below to match the name.\n",
    "# 4. Run this cell.\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import os # Import os to check paths\n",
    "\n",
    "# --- 1. SET YOUR PERMANENT MODEL PATH HERE ---\n",
    "# Find this path in your /kaggle/input/ directory after adding your model\n",
    "MODEL_INPUT_PATH = \"/kaggle/input/my-deepfake-detector-v1/\" \n",
    "# -----------------------------------------------\n",
    "\n",
    "# Define file paths\n",
    "WEIGHTS_FILE = \"efficientnet_b0_weights.pth\"\n",
    "MODEL_FILE = \"best_model_two_stream.pth\" # This file is inside the \"celeb_df_results_resnet_baseline\" folder\n",
    "                                         # If you saved it from there, the path might be:\n",
    "                                         # MODEL_FILE = \"celeb_df_results_resnet_baseline/best_model_two_stream.pth\"\n",
    "                                         # Please check your /kaggle/input/my-deepfake-detector-v1/ structure\n",
    "\n",
    "# Check if the path is correct\n",
    "if not os.path.exists(MODEL_INPUT_PATH):\n",
    "    print(f\"ðŸ›‘ ERROR: The path '{MODEL_INPUT_PATH}' does not exist.\")\n",
    "    print(\"Please check the 'Add Data' sidebar and update the MODEL_INPUT_PATH variable.\")\n",
    "else:\n",
    "    print(f\"âœ… Found model directory: {MODEL_INPUT_PATH}\")\n",
    "\n",
    "\n",
    "# --- 2. Re-define all necessary classes and functions ---\n",
    "try:\n",
    "    IMAGE_SIZE = CONFIG['image_size']\n",
    "    FRAME_COUNT = CONFIG['frame_count']\n",
    "except NameError:\n",
    "    print(\"CONFIG not found, using default values.\")\n",
    "    IMAGE_SIZE = 112\n",
    "    FRAME_COUNT = 16\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TwoStreamEfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        weights_path = os.path.join(MODEL_INPUT_PATH, WEIGHTS_FILE)\n",
    "        \n",
    "        efficientnet_temporal = models.efficientnet_b0(weights=None)\n",
    "        efficientnet_spatial = models.efficientnet_b0(weights=None)\n",
    "        \n",
    "        if pretrained:\n",
    "            print(f\"Loading pre-trained weights from local file: {weights_path}\")\n",
    "            try:\n",
    "                state_dict = torch.load(weights_path, map_location=DEVICE)\n",
    "                efficientnet_temporal.load_state_dict(state_dict)\n",
    "                efficientnet_spatial.load_state_dict(state_dict)\n",
    "                print(\"âœ… EfficientNet backbones loaded.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ðŸ›‘ FAILED to load weights: {e}\")\n",
    "        \n",
    "        self.temporal_backbone = nn.Sequential(*list(efficientnet_temporal.children())[:-1])\n",
    "        self.spatial_backbone = nn.Sequential(*list(efficientnet_spatial.children())[:-1])\n",
    "        self.feature_dim = 1280\n",
    "        self.temporal_lstm = nn.LSTM(self.feature_dim, 256, batch_first=True, bidirectional=True)\n",
    "        self.spatial_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fusion_classifier = nn.Sequential(\n",
    "            nn.Linear(512 + self.feature_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, frames, mosaic):\n",
    "        batch_size = frames.size(0)\n",
    "        num_frames = frames.size(1)\n",
    "        frames_flat = frames.view(batch_size * num_frames, *frames.shape[2:])\n",
    "        temporal_features = self.temporal_backbone(frames_flat)\n",
    "        temporal_features = temporal_features.view(batch_size, num_frames, self.feature_dim)\n",
    "        lstm_out, _ = self.temporal_lstm(temporal_features)\n",
    "        temporal_out = lstm_out[:, -1, :]\n",
    "        spatial_features = self.spatial_backbone(mosaic)\n",
    "        spatial_features = self.spatial_pool(spatial_features)\n",
    "        spatial_out = spatial_features.view(batch_size, self.feature_dim)\n",
    "        fused_features = torch.cat([temporal_out, spatial_out], dim=1)\n",
    "        output = self.fusion_classifier(fused_features)\n",
    "        return fused_features, output\n",
    "\n",
    "# --- 3. Re-define preprocessing functions ---\n",
    "\n",
    "def _sample_frames(video_path):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if not cap.isOpened() or total_frames < FRAME_COUNT:\n",
    "        if cap.isOpened(): cap.release()\n",
    "        print(f\"Error: Video {video_path} could not be opened or has < {FRAME_COUNT} frames.\")\n",
    "        return None\n",
    "    indices = np.linspace(0, total_frames - 1, FRAME_COUNT, dtype=int)\n",
    "    frames = []\n",
    "    for idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            frames.append(np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8))\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def _create_mosaic(frames):\n",
    "    mosaic = np.zeros((IMAGE_SIZE * 4, IMAGE_SIZE * 4, 3), dtype=np.uint8)\n",
    "    for idx, frame in enumerate(frames):\n",
    "        row = idx // 4\n",
    "        col = idx % 4\n",
    "        mosaic[row*IMAGE_SIZE:(row+1)*IMAGE_SIZE, col*IMAGE_SIZE:(col+1)*IMAGE_SIZE] = frame\n",
    "    return mosaic\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# --- 4. Load Our Trained Model ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING TRAINED DEEPFAKE DETECTOR...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = TwoStreamEfficientNet(pretrained=True).to(DEVICE)\n",
    "\n",
    "# --- This is the new, permanent path ---\n",
    "MODEL_PATH = os.path.join(MODEL_INPUT_PATH, MODEL_FILE)\n",
    "# --------------------------------------\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"âœ… Successfully loaded best model weights from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"   (Achieved {checkpoint['val_auc']:.4f} AUC during training)\")\n",
    "except Exception as e:\n",
    "    print(f\"ðŸ›‘ FAILED to load trained model weights from {MODEL_PATH}\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(\"   Please ensure the `MODEL_INPUT_PATH` is correct and you saved the model.\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# --- 5. Define the Inference Function ---\n",
    "\n",
    "def predict_video(video_path):\n",
    "    frames_np = _sample_frames(video_path)\n",
    "    if frames_np is None:\n",
    "        return \"Error\", 0.0, \"Could not process video.\"\n",
    "    mosaic_np = _create_mosaic(frames_np)\n",
    "    frames = torch.stack([val_transform(frame) for frame in frames_np])\n",
    "    mosaic = val_transform(mosaic_np)\n",
    "    frames = frames.unsqueeze(0).to(DEVICE)\n",
    "    mosaic = mosaic.unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, outputs = model(frames, mosaic)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        confidence_fake = probabilities[0][1].item()\n",
    "        confidence_real = probabilities[0][0].item()\n",
    "        \n",
    "        if confidence_fake > confidence_real:\n",
    "            prediction = \"FAKE\"\n",
    "            confidence = confidence_fake\n",
    "            caption = f\"The model is {confidence*100:.2f}% sure this video is FAKE.\"\n",
    "        else:\n",
    "            prediction = \"REAL\"\n",
    "            confidence = confidence_real\n",
    "            caption = f\"The model is {confidence*100:.2f}% sure this video is REAL.\"\n",
    "            \n",
    "        return prediction, confidence, caption\n",
    "\n",
    "# --- 6. Create the UI (ipywidgets) ---\n",
    "\n",
    "out = widgets.Output(layout={'border': '1px solid black', 'padding': '10px'})\n",
    "uploader = widgets.FileUpload(accept='video/*', multiple=False, description='Upload Video')\n",
    "\n",
    "def on_upload_change(change):\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        print(\"File uploaded. Processing...\")\n",
    "        \n",
    "        uploaded_file = list(change['new'].values())[0]\n",
    "        content = uploaded_file['content']\n",
    "        name = uploaded_file['name']\n",
    "        \n",
    "        # Save temporary file to /kaggle/working/\n",
    "        temp_path = f\"/kaggle/working/{name}\"\n",
    "        with open(temp_path, 'wb') as f:\n",
    "            f.write(content)\n",
    "            \n",
    "        print(f\"Running detection on {name}...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        prediction, confidence, caption = predict_video(temp_path)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        print(\"  DETECTION RESULTS\")\n",
    "        print(\"=\"*30)\n",
    "        print(f\"  Prediction: {prediction}\")\n",
    "        print(f\"  Confidence: {confidence*100:.2f}%\")\n",
    "        print(f\"  Analysis:   {caption}\")\n",
    "        print(f\"  (Time taken: {end_time - start_time:.2f} seconds)\")\n",
    "\n",
    "uploader.observe(on_upload_change, names='value')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  LIVE DEEPFAKE DETECTOR\")\n",
    "print(\"=\"*70)\n",
    "print(\"Upload a video file to run a real-time prediction.\")\n",
    "display(uploader, out)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3120670,
     "sourceId": 5380830,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
